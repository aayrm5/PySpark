{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Working_with_RDD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aayrm5/PySpark/blob/main/Working_with_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjuAVgrxmIo3"
      },
      "source": [
        "# **Working with RDD (Resilient Distributed Dataset)**\n",
        "\n",
        "**`Udemy Course: Best Hands-on Big Data Practices and Use Cases using PySpark`**\n",
        "\n",
        "**`Author: Amin Karami (PhD, FHEA)`**\n",
        "\n",
        "---\n",
        "\n",
        "**Resilient Distributed Dataset (RDD)**: RDD is the fundamental data structure of Spark. It is fault-tolerant (resilient) and immutable distributed collections of any type of objects.\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/rdd-programming-guide.html\n",
        "\n",
        "source: https://spark.apache.org/docs/latest/api/python/reference/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LWTJaC8mHL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f213dd42-5460-4a42-a16e-ad60249cd459"
      },
      "source": [
        "########## ONLY in Colab ##########\n",
        "!pip3 install pyspark\n",
        "########## ONLY in Colab ##########"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 64.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=cd48323a8adbc75a3a303b06c8d31420383ee293175aa62dbc6fa6f0c7ea765c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########## ONLY in Ubuntu Machine ##########\n",
        "# Load Spark engine\n",
        "!pip3 install -q findspark\n",
        "import findspark\n",
        "findspark.init()\n",
        "########## ONLY in Ubuntu Machine ##########"
      ],
      "metadata": {
        "id": "K-riUQ6WTHDl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linking with Spark\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "metadata": {
        "id": "e3pTfRiwTMeY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing Spark\n",
        "conf = SparkConf().setAppName(\"RDD_practice\").setMaster(\"local[*]\")\n",
        "sc = SparkContext(conf=conf)\n",
        "print(sc)"
      ],
      "metadata": {
        "id": "A_ALGTfeTPN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4e124d7-2b26-4da8-f3de-4967ac622462"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<SparkContext master=local[*] appName=RDD_practice>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sc.defaultParallelism"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX_uNHcVBTYq",
        "outputId": "9dd3bf79-186e-4155-a0a5-130296d2d334"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V3UuzFZ4BTIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1: Create RDDs and Basic Operations**\n",
        "# **There are two ways to create RDDs:**\n",
        "\n",
        "1.   Parallelizing an existing collection in your driver program\n",
        "2.   Referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat."
      ],
      "metadata": {
        "id": "quQ_GBpgWLRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate random data:\n"
      ],
      "metadata": {
        "id": "ILkhrdMMTu9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create RDD:\n"
      ],
      "metadata": {
        "id": "1n39Bv24XHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data distribution in partitions:\n"
      ],
      "metadata": {
        "id": "b8aOYoMLX7Er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print last partition\n"
      ],
      "metadata": {
        "id": "9EffFOyTYC18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count():\n"
      ],
      "metadata": {
        "id": "9TL1kG-Ceo6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first():\n"
      ],
      "metadata": {
        "id": "gZmfAahXeryY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top():\n"
      ],
      "metadata": {
        "id": "OnuGXcKLb8qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# distinct():\n"
      ],
      "metadata": {
        "id": "3xOj1w6teN_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# map():\n"
      ],
      "metadata": {
        "id": "qE0CJuhlZz1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter(): \n"
      ],
      "metadata": {
        "id": "r804677wamjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# flatMap():\n"
      ],
      "metadata": {
        "id": "9f--VFpvaqRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Descriptive statistics:\n"
      ],
      "metadata": {
        "id": "1LSPGU35gk-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapPartitions():\n"
      ],
      "metadata": {
        "id": "PEKBDcW1bvZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: Advanced RDD Transformations and Actions**"
      ],
      "metadata": {
        "id": "EGi2zdncaoHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# union():\n"
      ],
      "metadata": {
        "id": "bIKu4KMrdt1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intersection():\n"
      ],
      "metadata": {
        "id": "DmQ3bNUkeMVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find empty partitions\n"
      ],
      "metadata": {
        "id": "E2g0ep9M8GX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coalesce(numPartitions):\n"
      ],
      "metadata": {
        "id": "-AopsaZqehmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takeSample(withReplacement, num, [seed])\n"
      ],
      "metadata": {
        "id": "OFjDbelJeuoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# takeOrdered(n, [ordering])\n"
      ],
      "metadata": {
        "id": "_K41G_W9ezhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce():\n"
      ],
      "metadata": {
        "id": "sgBhaTdAeldY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduceByKey():\n"
      ],
      "metadata": {
        "id": "aj8-Q40_eXT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sortByKey():\n"
      ],
      "metadata": {
        "id": "Ii8M3qNMeaHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# countByKey()\n"
      ],
      "metadata": {
        "id": "2-WYDKd2e0qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# groupByKey():\n"
      ],
      "metadata": {
        "id": "bihcXC8DeUEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lookup(key):\n"
      ],
      "metadata": {
        "id": "5NzYUXEJhDM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cache:\n",
        "# By default, each transformed RDD may be recomputed each time you run an action on it.\n",
        "# However, you may also persist an RDD in memory using the persist (or cache) method,\n",
        "# in which case Spark will keep the elements around on the cluster for much faster access the next time you query it.\n"
      ],
      "metadata": {
        "id": "g9ThlLGO6z7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Persistence (https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence)\n"
      ],
      "metadata": {
        "id": "5zYpm9hpiqPc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}